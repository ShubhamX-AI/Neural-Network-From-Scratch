{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJUipfopxCsdInBi+Cs5uq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamX-AI/Neural-Network-From-Scratch/blob/main/Neural_Network_FromSractch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Neuron Example:** We start with a single neuron, demonstrating how it takes inputs, applies weights and biases, and outputs a value."
      ],
      "metadata": {
        "id": "JwHsK3GUuJO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "103-SmQet4Rz"
      },
      "outputs": [],
      "source": [
        "inputs = [1,3.5,2] #output of 3 neurons\n",
        "\n",
        "weights = [0.2, 1, 0.5]\n",
        "\n",
        "bias = 2\n",
        "\n",
        "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Neurons:** We create a network with multiple neurons, each processing the same input with different weights and biases."
      ],
      "metadata": {
        "id": "w3MpXq58uj9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1, 3.5, 2, 2.5] #Output From 4 neurons\n",
        "\n",
        "weights1 = [0.2, 1, 0.5, -0.1]\n",
        "weights2 = [0.2, 1, 0.5, -0.2]\n",
        "weights3 = [0.2, 1, 0.5, 0.23]\n",
        "\n",
        "bias1 = 2\n",
        "bias2 = 1\n",
        "bias3 = 1.5\n",
        "\n",
        "output = [\n",
        "    inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
        "    inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
        "    inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3\n",
        "]\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ivyXfzxIulxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Looping Through Neurons:** We use loops to iterate through calculations for each neuron, making the code more efficient for larger networks."
      ],
      "metadata": {
        "id": "smHsuFYXu_ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1, 3.5, 2, 2.5] # Inputs for the neurons\n",
        "\n",
        "# Weights for each neuron (each row corresponds to one neuron)\n",
        "weights = [[0.2, 1, 0.5, -0.1],\n",
        "           [0.2, 1, 0.5, -0.2],\n",
        "           [0.2, 1, 0.5, 0.23]]\n",
        "\n",
        "biases = [2, 1, 1.5] # Biases for each neuron\n",
        "\n",
        "neuron_outputs = [] # List to store the final output of each neuron\n",
        "\n",
        "# Iterate over each neuron's weights and bias\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "    neuron_output = 0  # Initialize the output for this neuron\n",
        "\n",
        "    # Calculate the weighted sum of inputs for the current neuron\n",
        "    for input_value, weight in zip(inputs, neuron_weights):\n",
        "        neuron_output += input_value * weight\n",
        "\n",
        "    # Add the bias to the weighted sum\n",
        "    neuron_output += neuron_bias\n",
        "\n",
        "    # Append the neuron's output to the output list\n",
        "    neuron_outputs.append(neuron_output)\n",
        "\n",
        "# Print the final output for all neurons\n",
        "print(neuron_outputs)\n"
      ],
      "metadata": {
        "id": "n3dEg120vAo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introducing NumPy:** We leverage NumPy's vectorized operations like dot product for faster computations."
      ],
      "metadata": {
        "id": "fwBCh-S-vGqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = [1, 3.5, 2, 2.5] # Inputs for the neurons\n",
        "\n",
        "# Weights for each neuron (each row corresponds to one neuron)\n",
        "weights = [[0.2, 1, 0.5, -0.1],\n",
        "           [0.2, 1, 0.5, -0.2],\n",
        "           [0.2, 1, 0.5, 0.23]]\n",
        "\n",
        "biases = [2, 1, 1.5] # Biases for each neuron\n",
        "\n",
        "output = np.dot(weights , inputs) + biases #Always put weights at first(During one batch) as it determines the number of neurons in output.\n",
        "                                           #Or else you will get a out of shape error. Here weights in metrix but the input is just a vector\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "v6b4l0QDvJWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Batch Input:** We modify the code to handle multiple data points (a batch) simultaneously, making it more practical for real-world scenarios."
      ],
      "metadata": {
        "id": "dLKjw1VgvVRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[1, 3.5, 2, 2.5],\n",
        "          [1, 2.5, 2, 0.5],\n",
        "          [1, 3.5, 1.2, 2.5]]\n",
        "\n",
        "# Weights for each neuron (each row corresponds to one neuron)\n",
        "weights = [[0.2, 1, 0.5, -0.1],\n",
        "           [0.2, 1, 0.5, -0.2],\n",
        "           [0.2, 1, 0.5, 0.23]]\n",
        "\n",
        "biases = [2, 1, 1.5] # Biases for each neuron\n",
        "\n",
        "output = np.dot( inputs, np.array(weights).T ) + biases #Always Transpose the weights after putting the input to match the deseaired number of output neuron. Or else you will get a out of shape error.\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "8VFMvmLDvZiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding 2 Layers**"
      ],
      "metadata": {
        "id": "NHfMwrhovmd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[1, 3.5, 2, 2.5],\n",
        "          [1, 2.5, 2, 0.5],\n",
        "          [1, 3.5, 1.2, 2.5]]\n",
        "\n",
        "# Weights for each neuron (each row corresponds to one neuron)\n",
        "weights = [[0.2, 1, 0.5, -0.1],\n",
        "           [0.2, 1, 0.5, -0.2],\n",
        "           [0.2, 1, 0.5, 0.23]]\n",
        "\n",
        "biases = [2, 1, 1.5] # Biases for each neuron\n",
        "\n",
        "layre1_output = np.dot( inputs, np.array(weights).T ) + biases\n",
        "\n",
        "weights2 = [[-0.2, 1, 3.5 ], #Because the input is comming from the previos 3 neurons\n",
        "           [0.2, 1, 0.5],\n",
        "           [0.2, 0.6, 0.5]]\n",
        "\n",
        "biases2 = [0.25, 1.6, 2.5]\n",
        "\n",
        "layre2_output = np.dot( layre1_output, np.array(weights2).T ) + biases2\n",
        "\n",
        "print(layre2_output)"
      ],
      "metadata": {
        "id": "QYk934ISv9dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Layers:** We create a **Dense_layer** class that encapsulates weights, biases, and the forward pass calculation for a single layer."
      ],
      "metadata": {
        "id": "QPOa5gThwAaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0) #Gives a fixed random number everytime\n",
        "\n",
        "X = [[1, 3.5, 2, 2.5],\n",
        "     [1, 2.5, 2, 0.5],\n",
        "     [1, 3.5, 1.2, 2.5]]\n",
        "\n",
        "class Dense_layer:\n",
        "  def __init__(self, n_inputs, n_neurons) -> None:\n",
        "     self.weight = 0.1* np.random.randn(n_inputs, n_neurons) #Size of input comming in to the size of neurons we wanna have\n",
        "     self.biases = np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "     self.output = np.dot(inputs , self.weight) + self.biases\n",
        "     return self.output\n",
        "\n",
        "layer1 = Dense_layer(4,5)\n",
        "layer1.forward(X)\n",
        "layer2 = Dense_layer(5,2) #Same input as the number of output neuron\n",
        "layer2.forward(layer1.output)#The output from layer1 has been fed into layer 2\n",
        "\n",
        "print(layer1.output)\n",
        "print(\"---------------\")\n",
        "print(layer2.output)"
      ],
      "metadata": {
        "id": "CF_wdDZDwIy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReLU Activation:** We implement the ReLU activation function, ensuring non-linearity in our network's learning process."
      ],
      "metadata": {
        "id": "orMj9EFOwNRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu_activation:\n",
        "    def relu(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "relu1 = Relu_activation()\n",
        "relu1.relu(layer1.output)\n",
        "print(relu1.output)\n"
      ],
      "metadata": {
        "id": "Ni5y8PizwcNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding the ReLU to the dense layer**"
      ],
      "metadata": {
        "id": "GSpS7Fvtwl4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0) #Gives a fixed random number everytime\n",
        "\n",
        "X = [[1, 3.5, 2, 2.5],\n",
        "     [1, 2.5, 2, 0.5],\n",
        "     [1, 3.5, 1.2, 2.5]]\n",
        "\n",
        "class Dense_layer:\n",
        "  def __init__(self, n_inputs, n_neurons) -> None:\n",
        "     self.weight = 0.1* np.random.randn(n_inputs, n_neurons) #Size of input comming in to the size of neurons we wanna have\n",
        "     self.biases = np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "     self.output = np.dot(inputs , self.weight) + self.biases\n",
        "     return self.output\n",
        "\n",
        "class Relu_acivation:\n",
        "  def relu(self, input):\n",
        "    self.output = np.maximum(0 , input) #Compare each element of the array with zero\n",
        "\n",
        "layer1 = Dense_layer(4,5)\n",
        "layer1.forward(X)\n",
        "layer2 = Dense_layer(5,2) #Same input as the number of output neuron\n",
        "layer2.forward(layer1.output)#The output from layer1 has been fed into layer 2\n",
        "\n",
        "activation = Relu_acivation()\n",
        "activation.relu(layer2.output)\n",
        "print(activation.output)"
      ],
      "metadata": {
        "id": "ZY87R2l9wsir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax Activation:** We introduce the Softmax function, commonly used for multi-class classification problems. It normalizes the output of the final layer, representing probabilities for each class.\n",
        "SoftMax = Input -> Exponentiate -> Normalize -> Output"
      ],
      "metadata": {
        "id": "-o4u8jgmwwoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import normcase\n",
        "X = [1, 2.5, -2, 0.5 , -0.1, 2 , 3 , 0.26, -1]\n",
        "\n",
        "import math\n",
        "E = math.e\n",
        "\n",
        "exponential_values = []\n",
        "normalized_value = []\n",
        "\n",
        "for value in X:\n",
        "  exponential_values.append(E**value)\n",
        "\n",
        "exponential_values2 = np.exp(X) # Or in place of this for loop you can use\n",
        "\n",
        "for value in exponential_values:\n",
        "  normalized_value.append(value / np.sum(exponential_values))\n",
        "\n",
        "normalized_value2 = exponential_values2/sum(exponential_values2) # Or in place of this for loop you can use\n",
        "\n",
        "print(exponential_values)\n",
        "print(\"-----------------\")\n",
        "print(exponential_values2)\n",
        "print(\"-----------------\")\n",
        "print(normalized_value)\n",
        "print(\"-----------------\")\n",
        "print(normalized_value2)\n",
        "print(\"-----------------\")\n",
        "print(np.sum(normalized_value2))"
      ],
      "metadata": {
        "id": "MCiQEnh9w0sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implemnting Softmax function**"
      ],
      "metadata": {
        "id": "MlYs7u_pxPRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0) #Gives a fixed random number everytime\n",
        "\n",
        "X = [[1, 3.5, 2, 2.5],\n",
        "     [1, 2.5, 2, 0.5],\n",
        "     [1, 3.5, 1.2, 2.5]]\n",
        "\n",
        "class Dense_layer:\n",
        "  def __init__(self, n_inputs, n_neurons) -> None:\n",
        "     self.weight = 0.1* np.random.randn(n_inputs, n_neurons) #Size of input comming in to the size of neurons we wanna have\n",
        "     self.biases = np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "     self.output = np.dot(inputs , self.weight) + self.biases\n",
        "     return self.output\n",
        "\n",
        "class Softmax_activation:\n",
        "  def softmax(self, input):\n",
        "    self.exponential_values = np.exp(input - np.max(input, axis=1 , keepdims=True)) #Subtracting the max value to prevent outflow and (axis = 1) -> work on rows and (keepdims = True) -> to keep the original shape and orientation\n",
        "    self.normalized_value = self.exponential_values/np.sum(self.exponential_values, axis=1 , keepdims=True)\n",
        "    self.output = self.normalized_value\n",
        "\n",
        "layer1 = Dense_layer(4,5)\n",
        "layer1.forward(X)\n",
        "layer2 = Dense_layer(5,3) #Same input as the number of output neuron. But the outpur number of neurons (3) can be changed to what ever neurn you like.\n",
        "layer2.forward(layer1.output)#The output from layer1 has been fed into layer 2\n",
        "\n",
        "activation = Softmax_activation()\n",
        "activation.softmax(layer2.output)\n",
        "print(activation.output)\n",
        "print(\"Sum \\n\", np.sum(activation.output , axis = 1 , keepdims = True))"
      ],
      "metadata": {
        "id": "KDs4kZmyxVHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Loss:** We explore Categorical Cross-Entropy, a common loss function that measures the difference between the predicted probabilities and the true target values. This loss guides the network's learning process towards minimizing the error."
      ],
      "metadata": {
        "id": "M5vt33Z1xwJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "softmax_output = [0.31515941, 0.39710045, 0.28774014]\n",
        "\n",
        "target_class = 1\n",
        "onehotencoded_output = [0,1,0]\n",
        "\n",
        "loss1 = -(math.log(softmax_output[0])*onehotencoded_output[0] + math.log(softmax_output[1])*onehotencoded_output[1] + math.log(softmax_output[2])*onehotencoded_output[2])\n",
        "\n",
        "loss2 = -(math.log(softmax_output[target_class])) #This is the simplier form as the most of the variable is multipird by 0\n",
        "\n",
        "print(loss1)\n",
        "print(loss2)"
      ],
      "metadata": {
        "id": "Bicx0j8PxxsR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}